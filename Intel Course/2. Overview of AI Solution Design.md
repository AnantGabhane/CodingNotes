## Value Driven Design in AI Solutions

- End Goal - Understanding how AI can understand and optimize operations of legacy systems

- AI systems are built of fundamental understanding that algorithms can extract information from data to form a comprehensive understanding of the system described by that data.
---
**Benifits of AI Systems:**

![alt text](image.png)

---
**Using AI into legacu operations :**
![alt text](lagacy_operations.png)
---
1. Cultivation :

- Using weather historical patterns to decide amount of water and fertilizer usage

2. Harvesting :
- Using drons and computer vision to determine harvesting condtions to improve product quality

3. Transportation :
- Using graph neural networks to optimize the pickup and drop off times for faster delivery

4. Processing :
- Using computer vision to improve product quality and removal of bad apples

5. Shipping :
- Using Graph neural networks to optimize the shipping routes to reduce cost 

6. Business :
- Using business intelligence applications to predict demand based on historical data
---
**AI Framework :**


![alt text](workflow.png)
![alt text](resource_for_poc.png)
![alt text](integration-with-legacy.png)
![alt text](financial-impact.png)
![alt text](user-learning-curve.png)


![alt text](maintaince.png)

Business is real winner here because it scores highest due to it's impact.

---
## Components / essentials of Traditional AI Solutions

A. ML Pipeline

B. Compute Infrastructure

C. User Interface

---

### **A. ML Pipeline - consists of**

#### 1. Data processing :
- Preparing and transforming raw data from machine learning 
- Tool used for data processing 
    - **Apache Spark** (used for big data processing)
    - **Pandas** (provides robust structure for efficient data manipulation and pre-processing)
    - **Apache Kafka** (distributed real time platform for real time data ingestion and processing, ensuring seamless data streaming for ML pipelines)


    - Data involved significantly impact performance and effectiveness of machine learning model
    -    Data processing steps :
        
         - **Collections** - gathering data through databases, API, files or scraping data from web 
         - **Data cleaning and pre-processing** :
                 
            - cleaning involves identifying data inconsistencies, errors and missing values in the data 
            - pre-processing involves formatting the data to make it suitable for model training. This helps in eliminating biaes, standardizing features and reduce the dimensionality of the data
    - **Feature engineering** - creating new features to capture relative patters and relationships between data    
 
---
#### 2. Model Development :
- Using processed datasets to train machine learning models
- Data scientists design and build predictive models based on processed data
- Stage involves choosing right model architecture, training the model using labeled data and evaluating its performance
- **Algorithmic selection** :
    - choosing the right architecture or algorithm that aligs the best with problem. Depends on data type and nature of the problem
    - commonly used algorithms are Linear regression, decision trees, random forest and support vector machines and neural networks
- **Training** :
    - training model using labeled data (training set)
    - model learns to find patterns and relationships with data by adjusting the parameters through optimization process 
    - Involves minimizing errors and losses such as mean square method or cross entropy
- **Validaiton** :
    - To check the performance of the model using evaluations matrics and validation data
    - validation data is used to assess the models generalization ability and detect the overfitting
    - Techniques such as cross validation and holdout validation are commenly employed during this process
---
#### 3. Inference - Using trained models to make predictions or generate assets 